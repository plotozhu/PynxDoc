# 概要说明
libp2p实现了Kademlia网络论文，可用于DHT网络的发现和处理。

# 网络连接
**注意**： libp2p中的网络连接由peerset管理，kad只监听网络连接的结果，并且根据这个结果来更新自身的bucket桶。这个过程是通过inject_connected来实现，inject_connected是NetworkBehaviour的一个特性。kad是NetworkBehaviuor的一个子behaviour,有关NetworkBehaviour的详细描述，参考[这篇文章](./libp2p-behaviour.md)

# 网络启动和发现
网络的启动是通过Kademlia::bootstrap()来实现的，这个函数就是向邻节点发送查询自身的命令，当邻节点收到此命令时，会把本节点的信息记录到邻节点表里，这样就实现了向邻节点汇报自身的信息。  
显然，网络启动的时候，另外还需一些bootnode节点，节点连接到这些bootnode节点，然后发送邻节点查询命令时，被bootnode节点所记录，其他节点也通过bootnode节点来发现自身。  
一旦网络通过bootnode连接到其他节点后，节点可以不再与bootnode连接。如果节点持久化记录了本次连接的节点，那么下次启动时，可以直接与这些节点连接，而不是再通过bootnode连接，这样既避免了bootnode节点的通信过于繁重，又能够防止因为bootnode被屏蔽导致网络无法建立。  
前面说过，lib-p2p网络没有对连接进行管理，而是由外部的peerset进行管理，因此peerset需要考虑kad网络的特征进行管理。在目前的实现中，是反向管理的，kad网络的每个bucket最多存放K个节点，如果属于同一个桶的更多的节点已经连接，将其放到Pending队列中而不是桶内
# 值存储和服务提供
从理论上说，值存储和服务提供都是统一的模式：节点定时重复向系统中推送一个键/值对，系统将根据健值哈希，将其推送到与该键值最近的节点上。另外一些节点可以发送读取命令，从与键最近的节点上获取到相应的值。其差别在于：
1. 键存储直接把Key/Value对推送到了离Key最近的节点上，也可以同时提供一个peerId，表明该值是由谁提供的。
2. 服务提供是把Key/PeerId对推送到了离key最近的节点上，实际Key对应的数据需要向该PeerId去读取。

两者对比，键存储适合提供小的value数据，而服务提供适合大的数据。

# 扩展使用
通过上述描述，我们可以明确网络是是通过协作方式对Key/Value(Provider)进行推送和读取的。我们考虑另外一种情况：多个节点提供一种服务。在kad中提供此类的服务，其方法为：
1. 在库中记录中key的多个provider
2. 当应用startprovider()或是provider_received时，向此Key添加一个provider
3. key的每个provider记录超时时间，当超时时，删除此provider
4. 当有查询需求时，把未过时的provider都返回出来

通过此方案，可以方便地解决多个节点提供一种服务的问题。每个节点只需要定时发送provider信息即可。

**另外一个问题：**
如果过多的节点提供同一个服务，是不是会导致读取的数据量太大？是否需要在应用层作一些处理？


# 关键参数
## K值
K值具有以下几个含义：
1. 每个桶里的最大节点数
2. 默认的复制因子，决定了：
   1. 一个查询请求响应的节点数
   2. 通多个请求返回的响应节点数
第二句话有点晦涩，直白的描述为：当发送一个查询请求时，得到K个节点后才返回给上层应用，无论该请求是只经过一次回应或是经过了多次转发后的回应。也就是说，当应用层发送一个查询请求时，如果本地记录了K个有效的节点，就直接回应，否则就向邻节点发送此请求，直到所有的回应结果+本地结果达到K个有效节点，就返回。（否则一直不返回？）

K的默认值是20
## ALPHA值
$\alpha$是Kademlia网络中的参数，它代表着一个请求通过多少个节点转发扩散出去。比如一个查询邻节点Key的请求，如果本地不够，那么就向$\alpha$个与Key更近的邻节点发送此请求，这$\alpha$查询自身的数据，如果不足，同样向自身记录的$\alpha$个更近邻节点发送请求，直到没有更近的为止。

目前$\alpha$值为3。 
**说明**
 经过实验证明，3是一个合理的数值，如果是2，失败的概率将会急剧上升；如果是4，通信数据量需求将会极大上升